\documentclass{article}
\usepackage[margin=2.3cm]{geometry} % Reduce margin size.
\usepackage{url}
\usepackage{amsmath}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{multicol}



% Don't indent any paragraph
\setlength{\parindent}{0pt}

\begin{document}

\title{CS $520$ Introduction to Artificial Intelligence \\ Homework 1 }
\date{October 14, 2015}
\author{Malihe Alikhani \\ \small{(RUID: 167001652)} \and Ana Echavarria Uribe \\ \small{(RUID: 167007648)}}

\maketitle

\section*{Part 0}

We designed and algorithm to create a maze like structure based on the algorithm presented in (\url{\http://www.migapro.com/depth-first-search/}) but with certain modifications. The idea is that when running a depth first search from a given cell select randomly one of its unseen neighbors that are two cells away in any direction (north, south, east or west). Move to that cell but blocking all of the unseen neighbors of the intermediate cell (the cell in between the current cell and the selected neighbor). This creates a maze like structure with no loops and one connected component.\\
Now, in order to add loops and creating more than one connected component we simply set random blocked cells as unblocked and random unblocked cells as blocked.
Examples of $5 \times 5$ grids generated using this algorithm are:

\newcommand{\bl}{\cellcolor{black!100}}
\begin{multicols}{2}
  \begin{center}
    % #.#.#
    % #.#.#
    % ...##
    % #.##.
    % #...#
    \begin{tabular}{|*{5}{c|}}
      \hline
      \bl & & \bl & & \bl \\ \hline
      \bl & & \bl & & \bl \\ \hline
      & & & \bl & \bl \\ \hline
      \bl &  & \bl & \bl & \\ \hline
      \bl & & & & \bl \\ \hline
    \end{tabular}
    
    \begin{tabular}{|*{5}{c|}}
      \hline
      & \bl & \bl & & \bl \\ \hline
      & & & \bl & \bl \\ \hline
      & \bl & & &  \\ \hline
      &  &  & \bl & \\ \hline
      & \bl & \bl & \bl & \bl \\ \hline
    \end{tabular}
    % .##.#
    % ...##
    % .#...
    % ...#.
    % .####
  \end{center}
\end{multicols}

\section*{Part 1}
\textbf{a.}\quad Since A* follows a path that has the lowest expected total cost, it will move to East rather than North to minimize the total expected cost. 

$$ g(\text{East})=1 \text{ and } h(\text{East}) = 2 \rightarrow f(\text{East})=1+2=3 $$
$$ g(\text{North})=1 \text{ and } h(\text{North})=4 \rightarrow f(\text{North})=1+4=5 $$

\textbf{b.}\quad The agent will not get stuck in an infinite loop because we do not visit a cell that has already been expanded in that iteration which means that every unblocked cell is expanded at most once. In this algorithm, we move that particular state to the closed list.\\
Whenever the path traversed by the agent is blocked, we recompute a new path. This makes sure we cover all the unblocked cells in the grid that are reachable by the agent. Unless there are infinite number of cells, the agent (A* algorithm) would find the goal state or would discover it is impossible to reach the goal because the there is no new path that gets the agent to the goal.\\
Also, the number of searches is abounded above by the number of unblocked cells. In each search, there is at least one new unblocked cell discovered so the total number of searches is bounded above by the total number of cells. Additionally, in each search the agent can move to each unblocked cell at most once (there are no loops in the moves) so the total number of moves is:
\begin{align*}
  \text{number of moves} &= \sum_{i = 0}^{\text{num of searches}} \text{number of moves in search } i\\
                         &\leq \text{maximum number of searches} \times \text{maximum number of moves in each search}\\
                         &= \text{number of unblocked cells} \times \text{number of unblocked cells}\\
                         &= \text{number of unblocked cells}^2\\
\end{align*}

\section*{Part 2}

The repeated forward A* was executed on 50 grid of size $101 \times 101$ using two different rules for selecting the next cell to expand when their $f = g + h$ value is the same: selecting the cell with the smaller $g$ value or with the larger value of $g$. The average number of cells expanded, searches (repetitions of the A* search), number of cells the agent moves and running time are shown in results in table~\ref{tab:comp_g}.

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{|*{5}{c|}}
      \cline{2-5}
      \multicolumn{1}{c|}{} & Num cells expanded & Num searches & Num moves & Running time (sec) \\ \hline
      Smaller g & 66,695  & 97 & 385 & 0.1608\\ \hline 
      Larger g & 8,164  & 95 & 376 & 0.0939 \\ \hline \hline
      Ratio Smaller/Larger & 8.1699&1.0185&1.0240&1.7124 \\ \hline
    \end{tabular}
  \end{center}
  \caption{Average statistics when running on 50 grids and breaking ties in favor of larger and smaller values of g.}
  \label{tab:comp_g}
\end{table}

It can be seen that, on average, the number of cells expanded when selecting the cell with the smaller value of $g$ is more than 8 times higher than the number of cells expanded when selecting the cells with the larger value of $g$. The running time, nonetheless is not 8 times higher; this could be because the time overhead of starting a search is so big that the ratio in the running time is only 1.7 times higher even if the search is doing 8 times the work. Additionally, we can see that in spite of the fact that the number of cells expanded is much larger, the algorithm still does around the same number of searches and the same number of moves for the agent; this would indicate that the number of cells expanded is larger for the tie breaking in favor of cells with smaller $g$ values because there are more cells being explored in every search rather than because there are more searches being done.\\
This difference in the number of cells expanded could be due to the fact than selecting a cell with a smaller $g$ rather than a cell with a larger $g$ implies that we are favoring the cell with the largest $h$ value. Since the $h$ value is a lower bound on the number of steps from that cell to the target, the cell that is being expanded is possibly farther away from the target than the other cell is. This could in the long run make us explore all the neighbors of that cell which are farther away and are not getting us to closer to the target.

\section*{Part 3}

The algorithms for the forward and backward A* were executed on the same 50 grids of size $101 \times 101$ and using the same initial cell for the agent and for the target cell. The results in terms of the average number of expanded cells, number of searches, number of moves of the agent and running time, along with the ratio comparison of these values are shown in table~ \ref{tab:back_for}.

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{|*{5}{c|}}
      \cline{2-5}
      \multicolumn{1}{c|}{} & Num cells expanded & Num searches & Num moves & Running time (sec) \\ \hline
      Backward A* & 455,896 & 96 & 373 & 0.7797\\ \hline 
      Forward A*  & 8,164  & 95 & 376 & 0.0879 \\ \hline \hline
      Ratio Backward/Forward & 55.845 & 1.0103 & 0.9923 & 8.8687 \\ \hline
    \end{tabular}
  \end{center}
  \caption{Average statistics when running on 50 grids and running backward and forward A*.}
  \label{tab:back_for}
\end{table}

% Think about how many *known* obstacles are near the goal and near the starting point.
We can see that when running the backward A* we explore on average 55 times more cells than with the forward A*. We can also see that because both algorithms do roughly the same number of searches, there are more cells being explored in each search with the backward algorithm than with the forward algorithm. This may be due to the fact that there are more blocked cells near the target than there are near the agent so each search needs to explore more cells to get from the target to the agent than the other way around.


\section*{Part 4}
Consider the coordinates of goal state G be $(G_x,G_y)$. Let cell A be $(x_1,y_1)$. The estimated path cost from cell $A$ to goal $G$ ($A-G$) is $$|G_x-x_1|+|G_y-y_1|.$$ Now assume the agent travels to the goal through a cell $B (x_2,y_2)$ which lies outside the path ($A-G$). The new path cost $A-B-G$ is $$|G_x- x_2|+|Gy-y2|+|x_2-x_1|+|y_2-y_1|$$
Compare the two path costs, $A-G$ and $A-B-G$.
\\
$$|G_x-x_2|+|x_2-x_1| \rightarrow  |G_x-x_1|$$(This is always true for any 3 values) $$ |G_y-y_2|+|y_2-y_1|   \rightarrow |G_y-y_1|$$(This is also always true for any 3 values)
Combining the two inequalities prove that estimated path cost $$(A-B-G) \rightarrow (A-G). $$So there can't be a shorter path to the goal through $B$. Therefore the initial path ($A-G$) estimated by the heuristic is the shortest path. Therefore Manhattan distances are consistent.
From the proof, we have $$(B-G)+(A-B)=(A-B-G) \rightarrow (A-G)$$  by triangle inequality.

\section*{Part 5}
The algorithms for the repeated forward A* and adaptive A* were executed on the same 50 grids of size $101 \times 101$ and using the same initial cell for the agent and for the target cell. The results in terms of the average number of expanded cells, number of searches, number of moves of the agent and running time, along with the ratio comparison of these values are shown in table~ \ref{tab:adaptive}.\\
We can see that the time of both algorithms is roughly the same but the adaptive A* explores 75\% of number of cells explored by repeated forward A*. As discussed in part 2, we believe that the time overhead for the search is big enough that the time difference between both algorithms doesn't differ that much. For that reason, we decide to do the comparison in terms of the number of explored cells rather than the running time.

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{|*{5}{c|}}
      \cline{2-5}
      \multicolumn{1}{c|}{} & Num cells expanded & Num searches & Num moves & Running time (sec) \\ \hline
      Adaptive A* & 6,062 & 96 & 378 & 0.0886\\ \hline 
      Repeated A*  & 8,164  & 95 & 376 & 0.0893 \\ \hline \hline
      Ration Adaptive A*/Repeated A* & 0.7426 & 1.0078 & 1.0068 & 0.9923 \\ \hline
    \end{tabular}
  \end{center}
  \caption{Average statistics when running on 50 grids and running repeated A* and adaptive A*.}
  \label{tab:adaptive}
\end{table}

We know that adaptive A* increases the values of $h$ as the algorithm is executed to include the new cells that have been discovered and are blocked. This allows it to decide to expand cells that may have greater manhattan distance than other cells with the same $g$ value because the other cell's $h$ value was increased with the adaptive A*. This increase occurs because the previous searches have discovered that the actual distance from that cell to the target is greater than its manhattan distance given the current explored cells in the board. Now, this change in the $h$ value that changes the cells expanded makes us reach the target faster and that is why the adaptive A* expands less cells than the repeated forward A*.

\section*{Part 6}
We suggest the following methods for decreasing the memory usage of our implementation. 

We can use a method when creating the states which does not  create all the states initially. We only create a state if it's going to be explored by the algorithm.

Another possibility is prevent storing f-value and h-values, which can both be calculated easily. The g-values can also be calculated by following tree-parents until reaching current start state. However, since this will increase the computation costs considerably, we do not include this improvement in our calculation. 

 Another improvement could be instead of storing the counter as search variable, a boolean variable can be used to check if the state is visited in the current iteration.
\end{document}